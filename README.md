# Fancryrag Neo4j GraphRAG Baseline

This project bootstraps a Neo4j-backed GraphRAG pipeline using Astral's `uv` package manager.

## Quick Start

```bash
# Install dependencies (creates .venv/)
uv sync

# Copy base configuration and fill in real credentials
cp .env.example .env.local
# ...edit .env.local with embedding, OAuth, and Neo4j secrets...

# Build the MCP image using the frozen uv.lock
docker compose build mcp

# Launch Neo4j and the MCP server on the rag-net bridge
make up

# Populate indexes and ingest a sample document
make index
make fulltext-index
printf 'Alice founded Acme Corp in 2012. Bob joined in 2015.' > sample.txt
make ingest f=sample.txt

# Inspect graph counts
make counts
```

Stop the stack with `make down`. Follow service logs with `make logs`.

`uv.lock` is the canonical dependency lockfile for installs and Docker builds.
`requirements.lock` is a derived snapshot used by diagnostics/tests and is
generated by `scripts/bootstrap.sh` (prefers `uv pip freeze` when available).

To publish an image, tag and push the compose-built artifact with the Git SHA digest:

```bash
docker tag fancryrag-mcp:local ghcr.io/<org>/fancryrag-mcp:$(git rev-parse --short HEAD)
docker push ghcr.io/<org>/fancryrag-mcp:$(git rev-parse --short HEAD)
```

If the default host bindings (Neo4j HTTP 22010, Neo4j Bolt 22011, MCP 22012) collide
with other services, override them before starting the stack:

```bash
NEO4J_HTTP_PORT=17474 NEO4J_BOLT_PORT=17687 MCP_PUBLISHED_PORT=18080 make up
```

Configure secrets in `.env.local` before running ingestion. Chat/semantic extraction
uses OpenAI, while embeddings are routed to the local OpenAI-compatible server. For
local embeddings (e.g., `localhost:20010`), set:

```ini
EMBEDDING_API_BASE_URL=http://localhost:20010/v1
EMBEDDING_API_KEY=<token or dummy if not required>
EMBEDDING_MODEL=<local embedding model name>
```

Local embeddings require `EMBEDDING_API_BASE_URL`; the pipeline fails fast if it is
missing. If the local endpoint uses HTTP, set `OPENAI_ALLOW_INSECURE_BASE_URL=true`.

Semantic extraction uses structured output by default. You can control the response
format and retry behavior with these env vars:

```ini
# json_schema (default), json_object, or off
OPENAI_SEMANTIC_RESPONSE_FORMAT=json_schema
# enforce strict JSON schema validation (default true)
OPENAI_SEMANTIC_SCHEMA_STRICT=true
# retry count for semantic extraction format errors (default 1)
OPENAI_SEMANTIC_MAX_RETRIES=1
# write sanitized semantic failure artifacts (default false)
OPENAI_SEMANTIC_FAILURE_ARTIFACTS=false
```

If you are using OpenAI project-scoped keys (`sk-proj-...`), also capture the project
identifier issued by OpenAI:

```
OPENAI_API_KEY=sk-proj-...
OPENAI_PROJECT=proj_...
```

Project keys require the `project` field so the Python SDK can route requests to the
correct workspace.

For hybrid search, set the index configuration variables (sensible defaults shown):

```ini
INDEX_NAME=text_embeddings
FULLTEXT_INDEX_NAME=chunk_text_fulltext
FULLTEXT_LABEL=Chunk
FULLTEXT_PROPERTY=text
FULLTEXT_READY_ATTEMPTS=10
FULLTEXT_READY_DELAY=3
```

The full-text index script is idempotent; rerun `make fulltext-index` after ingestion
jobs or schema changes to keep lexical search synchronized with vector metadata.

`FULLTEXT_READY_ATTEMPTS` and `FULLTEXT_READY_DELAY` tune how long the provisioning
script waits for Neo4j to accept connections before failing. Defaults cover local
Docker startup, but tighten them for pre-warmed environments or extend for slower
clusters.

### Container health & readiness

Once `make up` completes, confirm the MCP container is healthy and emitting
structured logs:

```bash
curl http://localhost:8080/mcp/health
make logs  # tail Neo4j + MCP output, Ctrl+C to exit
```

The Docker healthcheck mirrors the `/mcp/health` endpoint, so `docker compose ps`
will report the `mcp` service as `healthy` when this check passes.

### Authorization for local automation

Production deployments must supply Google OAuth tokens. For local-only runs you can
disable auth by setting `MCP_AUTH_REQUIRED=false` in `.env.local`, which skips the
OAuth requirements entirely. If you still want auth for local/CI smoke tests, set
`MCP_STATIC_TOKEN` and keep OAuth enabled:

```
MCP_STATIC_TOKEN=dev-smoke-token
GOOGLE_OAUTH_CLIENT_ID=dummy
GOOGLE_OAUTH_CLIENT_SECRET=dummy
GOOGLE_OAUTH_REQUIRED_SCOPES=openid
```

```
curl -H 'Authorization: Bearer dev-smoke-token' \
     -H 'Content-Type: application/json' \
     -d '{"query":"container","top_k":3}' \
     http://localhost:8080/mcp/search
```

Leave `MCP_STATIC_TOKEN` unset outside controlled testing so Google OAuth remains
enforced.

### End-to-end container smoke test

An automated pytest scenario exercises the full container workflow (image build,
Compose startup, Neo4j seeding, `/mcp/health`, and `POST /mcp/search`). Run it
inside the dedicated smoke Compose definition so the check stays isolated from
your developer containers:

```bash
make smoke
```

The smoke stack spins up a stub embeddings service on the internal Compose
network, provisions Neo4j indexes, and issues a hybrid search using a static MCP
token. Logs are captured automatically, and the environment is torn down with
`docker compose down --volumes` when the run completes.

### Image hygiene & secret scanning

Run `make scan-image` whenever Docker inputs change to rebuild `fancryrag-mcp:local`,
emit the resulting image digest, and execute a Trivy secret scan. The repository
also runs this check automatically via `.github/workflows/container-scan.yml` for
pull requests that touch container or Compose assets.

## FastMCP Hybrid Server

Story 1.2 introduces a Google OAuth-protected FastMCP server that fronts the
`HybridCypherRetriever`. Configure the additional environment variables (see
`.env.example` for defaults):

- `MCP_BASE_URL`: Public base URL used during OAuth callbacks (`https://...`).
- `MCP_SERVER_HOST`, `MCP_SERVER_PORT`, `MCP_SERVER_PATH`: Local binding for the
  HTTP transport; defaults to `0.0.0.0:8080` and `/mcp`.
- `GOOGLE_OAUTH_CLIENT_ID`, `GOOGLE_OAUTH_CLIENT_SECRET`: Credentials issued via
  Google Cloud Console.
- `GOOGLE_OAUTH_REQUIRED_SCOPES`: Comma-separated scopes. The baseline requests
  `openid` and `userinfo.email`.
- `HYBRID_RETRIEVAL_QUERY_PATH`: Path to the Cypher projection appended after the
  hybrid search prelude. The default file `queries/hybrid_retrieval.cypher` returns
  the node, its text, and the combined score.
- `EMBEDDING_MODEL`, `EMBEDDING_TIMEOUT_SECONDS`, `EMBEDDING_MAX_RETRIES`: Tuning
  knobs for the OpenAI-compatible embedding client that powers query embeddings.

After populating `.env.local`, use `make up` to run the containerized server (the
Dockerfile installs the project with `uv sync --frozen`). For local debugging you
can still execute the entrypoint directly:

```bash
uv run python servers/mcp_hybrid_google.py
```

Structured JSON logs announce startup, incoming tool invocations, embedding
latencies, and retries. The `/mcp/search` tool returns both normalized vector and
full-text scores so downstream clients (e.g., ChatGPT) can reason about result
ranking. When semantic enrichment is enabled during ingestion, `/mcp/search`
also includes optional `semantic_nodes` and `semantic_relationships` arrays for
each result. Use the `/mcp/fetch` tool to retrieve a specific node by
`elementId` and see its metadata.

## Neo4j Container Layout

`docker-compose.yml` now provisions both Neo4j and the FastMCP service on the
shared `rag-net` bridge. Neo4j exposes the existing volumes (`/data`, `/logs`,
`/plugins`) and a healthcheck so the MCP container only starts once Bolt is
reachable. The `mcp` service builds from the local `Dockerfile`, injects secrets
via `env_file`, performs a `/mcp/health` probe, and publishes port 8080. Use
`make up` / `make down` for lifecycle management, and re-run `make index` whenever
the embedding dimension changes (default: 1024 for the local model).
