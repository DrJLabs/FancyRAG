# Story 2.2: OpenAI Readiness Probe CLI

## Status
Ready for Review

## Story
**As a** GraphRAG platform operator,
**I want** a non-destructive OpenAI readiness probe that validates chat and embedding settings end-to-end,
**so that** I can confirm model guardrails, telemetry, and fallbacks work before running vector-heavy pipelines.

## Acceptance Criteria
1. Provide a CLI probe (`python -m cli.diagnostics openai-probe`) that issues a lightweight chat completion and embedding request using the configured defaults, persists a sanitized report (e.g., `artifacts/openai/probe.json`), and confirms guardrails from `OpenAISettings` before exiting 0 on success. [Source: docs/prd.md#epic-2---models--vectors] [Source: docs/architecture/overview.md#workspace-verification] [Source: docs/architecture.md#openai-api]
2. The probe emits Prometheus-compatible metrics (e.g., `artifacts/openai/metrics.prom`) using existing telemetry helpers, records fallback usage (`gpt-4o-mini`) in structured logs, and redacts sensitive fields in both outputs. [Source: docs/prd.md#epic-2---models--vectors] [Source: docs/architecture.md#cli-orchestrator] [Source: docs/architecture/coding-standards.md#logging-guidance]
3. Failures such as invalid chat model, embedding dimension mismatch, or API errors implement exponential backoff for 429/`RateLimitError` responses, surface token-budget remediation text, exit with non-zero codes, and reference next steps without leaking secrets. [Source: docs/stories/2.1.openai-model-guardrails.md#dev-notes] [Source: docs/architecture.md#openai-api] [Source: docs/architecture/coding-standards.md#critical-rules]
4. Update documentation to describe running the probe, interpreting metrics (including default histogram buckets covering 100 ms–5 s), and preparing for downstream vector upsert tasks; add automated tests that stub OpenAI to cover success, fallback, and failure paths with artifact/metric assertions. [Source: docs/prd.md#epic-2---models--vectors] [Source: docs/architecture/overview.md#workspace-verification] [Source: docs/architecture/coding-standards.md#testing-expectations]

## Tasks / Subtasks
- [x] Extend `src/cli/diagnostics.py` (or dedicated probe module) with an `openai-probe` subcommand that loads `OpenAISettings`, performs chat + embedding calls, writes sanitized JSON output under `artifacts/openai/probe.json`, and returns exit code 0 on success. (AC: 1) [Source: docs/architecture.md#cli-orchestrator] [Source: docs/architecture/source-tree.md#source-tree-blueprint]
  - [x] Capture actor/model metadata in the report while omitting prompts or credentials per logging standards. (AC: 1) [Source: docs/architecture/coding-standards.md#critical-rules]
  - [x] Reuse diagnostics redaction utilities (e.g., `src/cli/sanitizer.py`) to scrub artifacts/logs and document the shared helper for future probes. (AC: 1,2) [Source: docs/stories/2.1.openai-model-guardrails.md#dev-notes] [Source: docs/architecture/coding-standards.md#critical-rules]
- [x] Integrate existing `OpenAIMetrics` helpers so the probe exports Prometheus text to `artifacts/openai/metrics.prom`, defines default histogram buckets (100 ms–5 s) aligned with latency SLOs, annotates fallback usage, and emits structured logs with redacted payloads. (AC: 2) [Source: docs/architecture/coding-standards.md#logging-guidance] [Source: docs/architecture.md#openai-api]
  - [x] Add pytest assertions that the configured histogram buckets are registered and exported. (AC: 2) [Source: docs/architecture/coding-standards.md#testing-expectations]
- [x] Implement failure handling that maps guardrail violations, API errors, and 429/`RateLimitError` responses to human-readable remediation guidance using exponential backoff before surfacing non-zero exit codes while preserving secret hygiene. (AC: 3) [Source: docs/stories/2.1.openai-model-guardrails.md#dev-notes] [Source: docs/architecture/coding-standards.md#critical-rules]
  - [x] Ensure CLI output and persisted artifacts include token-budget guidance after backoff exhaustion. (AC: 3) [Source: docs/architecture/coding-standards.md#critical-rules]
- [x] Document probe workflow in `docs/architecture/overview.md` (or dedicated operations shard), covering when to run it, expected outputs, fallback indicators, and how metrics support the upcoming vector upsert service. (AC: 4) [Source: docs/architecture/overview.md#workspace-verification] [Source: docs/architecture.md#vector-upsert-service]
- [x] Add pytest coverage (unit + integration-style) that stubs OpenAI chat/embedding responses to validate success, fallback, and error scenarios, asserting artifact/metric contents, histogram bucket configuration, and log redaction (including negative cases that fail if sensitive secrets appear). (AC: 4) [Source: docs/architecture/coding-standards.md#testing-expectations]

## Dev Notes

### Previous Story Insights
- Story 2.1 delivered `OpenAISettings` guardrails and telemetry helpers; the probe must reuse those utilities instead of reinventing configuration or metrics plumbing. [Source: docs/stories/2.1.openai-model-guardrails.md#dev-notes]
- Prior diagnostics workflow already masks secrets and writes artifacts; mirror its sanitization approach to keep reports shareable. [Source: docs/stories/2.1.openai-model-guardrails.md#dev-notes]

### Data Models
- `VectorRecord` payloads remain 1536-dimensional, so embedding validation should align with the vector schema expected by downstream upsert logic. [Source: docs/architecture.md#vectorrecord]

### API Specifications
- OpenAI chat requests use `POST /v1/chat/completions` and embeddings use `POST /v1/embeddings`; both require bearer auth and respect retry/backoff caps. [Source: docs/architecture.md#openai-api]
- Probe must operate within documented retry limit (≤5) to avoid runaway cost while still surfacing telemetry. [Source: docs/architecture/coding-standards.md#critical-rules]

### Component Specifications
- CLI orchestrator houses operational subcommands; implement the probe as an additional diagnostics entry point consistent with existing CLI patterns. [Source: docs/architecture.md#cli-orchestrator]
- Telemetry module already exposes `OpenAIMetrics`; leverage it to avoid duplicate metric definitions. [Source: docs/architecture.md#cli-orchestrator]

### File Locations
- Place new CLI code under `src/cli/` and corresponding tests under `tests/unit/cli/`; artifacts belong under `artifacts/openai/` as documented for diagnostics outputs. [Source: docs/architecture/source-tree.md#source-tree-blueprint]

### Testing Requirements
- Use `pytest` with faked OpenAI clients to validate success, fallback, and failure flows without making network calls. [Source: docs/architecture/coding-standards.md#testing-expectations]
- Ensure tests assert metric export, artifact schema, and log redaction to guard against regressions. [Source: docs/architecture/coding-standards.md#critical-rules]

### Technical Constraints
- Continue targeting Python 3.12 and existing dependency set (openai 1.x, structlog, prometheus-client) when implementing the probe. [Source: docs/architecture/tech-stack.md#technology-stack]

### Project Structure Notes
- Align probe implementation with documented source tree; update blueprint if new files (e.g., `src/cli/probe.py`) are introduced. [Source: docs/architecture/source-tree.md#source-tree-blueprint]

### Documentation Alignment
- Epic 2 expects operators to understand cost and latency trade-offs; documentation should highlight probe outputs and link to telemetry dashboards. [Source: docs/prd.md#epic-2---models--vectors]
- Connect probe instructions to vector upsert preparation so teams know when to run validations before ingestion. [Source: docs/architecture.md#vector-upsert-service]

### Secrets Handling
- Reports and logs must mask API keys and prompt contents, reusing sanitization routines from diagnostics (`src/cli/sanitizer.py`) to maintain compliance and parity across probes. [Source: docs/architecture/coding-standards.md#critical-rules]

### Data Pipelines
- Probe artifacts should feed into vector upsert readiness checks, ensuring embeddings and latency metrics meet expectations before pipelines execute. [Source: docs/architecture.md#vector-upsert-service]

### Resolved Questions
- ✅ Skip-live mode implemented via `--skip-live` flag for air-gapped CI environments.

## Testing
- Unit test: stub OpenAI responses to exercise successful probe run, verifying artifact schema, metric export, and sanitized logs. (AC 1,2)
- Unit test: configure fallback chat model (`gpt-4o-mini`) and confirm logs/metrics flag the override while still exporting sanitized outputs. (AC 2)
- Unit test: validate Prometheus histogram buckets (100 ms–5 s) are registered and exported, including negative cases that fail if configuration drifts. (AC 2)
- Unit test: simulate embedding dimension mismatch to ensure probe surfaces remediation guidance and non-zero exit code. (AC 3)
- Unit test: provoke 429/`RateLimitError` responses to verify exponential backoff behavior and token-budget messaging in CLI output/artifacts. (AC 3)
- Unit test: assert reusable redaction helpers prevent secrets from appearing in logs/artifacts, including failure cases seeded with test credentials. (AC 1,4)
- Integration-style test: run probe entry point with stubbed clients to verify documentation instructions (artifact paths, metrics file) remain accurate. (AC 4)

## Change Log

| Date       | Version | Description                             | Author |
|------------|---------|-----------------------------------------|--------|
| 2025-09-25 | 0.1     | Initial draft of Story 2.2 created      | Bob    |

## Dev Agent Record
### Agent Model Used
- Codex GPT-5 (dev persona)

### Debug Log References
- `pytest tests/unit/cli/test_sanitizer.py tests/unit/cli/test_telemetry.py tests/unit/cli/test_openai_probe.py tests/unit/cli/test_diagnostics.py`
- `pytest tests/integration/cli/test_openai_probe_cli.py`

### Completion Notes List
- Implemented OpenAI readiness probe CLI subcommand with sanitized reporting, exponential backoff, and metrics export.
- Centralized redaction helpers in `src/cli/sanitizer.py` and updated telemetry buckets (100 ms–5 s) to satisfy SLO alignment.
- Documented probe workflow/setup in `docs/architecture/overview.md` and added unit/integration pytest coverage for success, fallback, rate-limit, redaction, and metrics scenarios.

### File List
- src/cli/diagnostics.py
- src/cli/sanitizer.py
- src/cli/telemetry.py
- docs/architecture/overview.md
- docs/qa/assessments/2.2-po-validation-20250925.md
- docs/qa/assessments/2.2-researcher-validation-20250925.md
- tests/unit/cli/test_sanitizer.py
- tests/unit/cli/test_telemetry.py
- tests/unit/cli/test_openai_probe.py
- tests/integration/cli/test_openai_probe_cli.py
- tests/unit/cli/test_diagnostics.py

## QA Results
### Risk Profile
- Pending

### Test Design
- Pending

### Traceability
- 2025-09-26: docs/qa/assessments/2.2-trace-20250925.md (all acceptance criteria fully covered)

### NFR Assessment
- 2025-09-26: docs/qa/assessments/2.2-nfr-20250925.md (security/performance/reliability PASS)

### PO Validation
- 2025-09-25: docs/qa/assessments/2.2-po-validation-20250925.md (approved with follow-up actions)

### Gate Status
- 2025-09-26: docs/qa/gates/2.2-openai-readiness-probe-cli.yml (PASS)

## 🔬 Research & Validation Log
- Pending
