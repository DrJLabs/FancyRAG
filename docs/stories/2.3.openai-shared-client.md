# Story 2.3: OpenAI Shared Client Abstraction

## Status
Draft

## Story
**As a** GraphRAG pipeline developer,
**I want** a reusable OpenAI client wrapper that centralises guardrails, telemetry, and fallbacks,
**so that** every CLI workflow (diagnostics, ingest, and future vector pipelines) enforces the same reliability contracts without duplicating code.

## Acceptance Criteria
1. Provide a shared OpenAI client module (e.g., `src/cli/openai_client.py`) that centralises chat and embedding helpers using `OpenAISettings`, `OpenAIMetrics`, structured redaction, and exponential backoff; chat helpers must invoke the Chat Completions API (`POST /v1/chat/completions`) and enforce the GPT-4.1-mini baseline allowlist (with `gpt-4o-mini` as the documented fallback), while embedding helpers guard the default 1536 dimension and any opt-in overrides. [Source: docs/prd.md#epic-2---models--vectors] [Source: docs/architecture.md#openai-api] [Source: docs/architecture/coding-standards.md#critical-rules]
2. Refactor the readiness probe to consume the shared client (Chat Completions for dialog, embeddings helper for vectors) without changing its CLI surface, artifacts, or metrics outputs, and publish migration guidance for ingest/retrieval pipelines that will adopt the wrapper. [Source: docs/architecture/overview.md#openai-readiness-probe] [Source: docs/architecture/source-tree.md#source-tree-blueprint]
3. Surface configurable retries, `Retry-After` aware backoff, fallback toggles, and embedding-dimension overrides via settings with documentation that explains cost/latency expectations for baseline versus fallback models and when to adjust overrides. [Source: docs/prd.md#epic-2---models--vectors] [Source: docs/architecture/coding-standards.md#critical-rules]
4. Add unit and integration tests that stub chat completions and embeddings calls to validate fallback usage, embedding overrides, `Retry-After` handling, metrics emission, and redacted logs when the shared client is invoked from both direct helpers and CLI entrypoints. [Source: docs/architecture/coding-standards.md#testing-expectations] [Source: docs/stories/2.2.openai-readiness-probe.md#testing]

## Tasks / Subtasks
- [x] Introduce `src/cli/openai_client.py` (or equivalent) encapsulating Chat Completions and embedding operations, reusing `OpenAISettings`, `OpenAIMetrics`, redaction helpers, and exponential backoff. (AC: 1) [Source: docs/architecture/source-tree.md#source-tree-blueprint] [Source: docs/architecture/coding-standards.md#critical-rules]
  - [x] Enforce the GPT-4.1-mini allowlist and only permit `gpt-4o-mini` when `OPENAI_ENABLE_FALLBACK=true`, raising structured errors for any other chat model inputs. (AC: 1) [Source: docs/prd.md#epic-2---models--vectors]
  - [x] Ensure embedding responses route through `ensure_embedding_dimensions()` while supporting the opt-in override setting without breaking downstream expectations. (AC: 1) [Source: docs/architecture.md#vectorrecord]
- [x] Update `src/cli/diagnostics.py` readiness probe implementation to delegate to the shared client (Chat Completions for chat, shared helper for embeddings) while keeping CLI flags, report schema, and metrics files untouched. (AC: 2) [Source: docs/architecture/overview.md#openai-readiness-probe]
  - [x] Provide migration notes in `docs/architecture/overview.md` describing how ingest and retrieval pipelines should adopt the wrapper and dimension override guidance. (AC: 2,3) [Source: docs/prd.md#epic-2---models--vectors]
- [x] Expose configuration for retries, `Retry-After` aware backoff, fallback toggles, and embedding-dimension overrides in `OpenAISettings` (or adjacent config) and document operator guidance covering latency/cost trade-offs. (AC: 3) [Source: docs/architecture.md#openai-api] [Source: docs/prd.md#epic-2---models--vectors]
- [x] Add pytest coverage (unit + integration-style) that stubs chat completions and embedding calls for success, fallback, rate-limit (`Retry-After`), and dimension override scenarios, asserting metrics, logs, and artifacts stay redacted. (AC: 4) [Source: docs/architecture/coding-standards.md#testing-expectations]
  - [x] Verify tests exercise shared client usage from both direct helpers and the readiness probe entrypoint to prevent regressions. (AC: 4) [Source: docs/stories/2.2.openai-readiness-probe.md#testing]

## Dev Notes

### Previous Story Insights
- Story 2.2 emphasised reusing OpenAI guardrails, telemetry, and sanitisation; centralising these helpers avoids future divergence across CLI commands. [Source: docs/stories/2.2.openai-readiness-probe.md#dev-notes]
- Story 2.1 introduced `OpenAISettings` and embedding validation helpers that should remain the single source of truth. [Source: docs/stories/2.1.openai-model-guardrails.md#dev-notes]

### Data Models
- `VectorRecord` payloads expect 1536-length embeddings; the shared client must enforce dimensions before downstream vector upsert work begins. [Source: docs/architecture.md#vectorrecord]

### API Specifications
- OpenAI chat continues to use `POST /v1/chat/completions` while embeddings use `POST /v1/embeddings`; both require bearer auth, GPT-4.1 allowlist enforcement, exponential backoff, and rate-limit resilience. [Source: docs/architecture.md#openai-api]

### Component Specifications
- CLI orchestrator remains the integration hub; new wrappers should live alongside existing CLI utilities for consistent import paths. [Source: docs/architecture/overview.md#high-level-components]
- Metrics must continue flowing through Prometheus-compatible helpers to feed operations dashboards. [Source: docs/architecture/overview.md#openai-readiness-probe]

### File Locations
- Place new client code under `src/cli/` and tests under `tests/unit/cli/` / `tests/integration/cli/` to match the documented source tree. [Source: docs/architecture/source-tree.md#source-tree-blueprint]

### Testing Requirements
- Use pytest with faked Chat Completions and embedding clients to validate success, fallback, rate-limit, and redaction scenarios without live API calls. [Source: docs/architecture/coding-standards.md#testing-expectations]
- Ensure integration tests execute the readiness probe via CLI entrypoints to confirm shared-client integration safety and metric stability. [Source: docs/stories/2.2.openai-readiness-probe.md#testing]

### Technical Constraints
- Continue targeting Python 3.12 and the existing dependency set (`openai` 1.x, `structlog`, `prometheus-client`). [Source: docs/architecture/tech-stack.md#technology-stack]

### Project Structure Notes
- If additional shared modules are introduced, update `docs/architecture/source-tree.md` to reflect the new layout so future stories stay aligned. [Source: docs/architecture/source-tree.md#source-tree-blueprint]

### Documentation Alignment
- Epic 2 requires cost/latency guidance for baseline vs fallback models; update operations docs with SLO expectations and toggle behaviour. [Source: docs/prd.md#epic-2---models--vectors]
- Architecture shards should document the shared client interface, Chat Completions usage, and embedding dimension override guidance so downstream teams stay aligned. [Source: docs/architecture/overview.md#openai-readiness-probe]

### Secrets Handling
- All logs/artifacts emitted by the shared client must continue to redact API keys, prompts, and tokens to satisfy compliance rules. [Source: docs/architecture/coding-standards.md#critical-rules]

### Data Pipelines
- The wrapper should prepare for reuse by upcoming vector upsert flows, ensuring embeddings remain idempotent and retry-aware before handing data to Qdrant. [Source: docs/architecture.md#vector-upsert-service]

### Monitoring & Observability
- Maintain Prometheus histogram buckets (100â€¯msâ€“5â€¯s) and token counters so dashboards retain continuity after the refactor. [Source: docs/architecture/overview.md#openai-readiness-probe]
- Emit rate-limit counters/timers (including parsed `Retry-After` seconds) so operations can monitor when the shared client falls back or delays requests. [Source: docs/architecture/coding-standards.md#logging-guidance]

### Open Questions
- Async client strategy is tracked via the follow-up task below; record the decision outcome before QA sign-off. [Source: docs/architecture.md#openai-api]

## Testing
- Unit tests: exercise Chat Completions and embedding helpers with successful responses, verifying telemetry counters/histograms and redacted logs. (AC 1,4)
- Unit tests: simulate fallback (`gpt-4o-mini`) usage, embedding dimension overrides, and invalid models to ensure guardrails raise actionable errors. (AC 1,4)
- Unit tests: trigger rate-limit exceptions with `Retry-After` headers to validate exponential backoff, retry caps, and remediation messaging. (AC 1,3,4)
- Integration-style test: invoke `python -m cli.diagnostics openai-probe` with the shared client stubbed to confirm Chat Completions wiring keeps artifacts/metrics unchanged and emits backoff metrics. (AC 2,4)
- Test execution: `pytest tests/unit/cli/test_openai_client.py tests/unit/cli/test_openai_probe.py tests/unit/config/test_openai_settings.py tests/integration/cli/test_openai_probe_cli.py`

## Change Log
| Date       | Version | Description                        | Author |
|------------|---------|------------------------------------|--------|
| 2025-09-26 | 0.1     | Initial draft of Story 2.3 created | Bob    |
| 2025-09-27 | 0.2     | Updated acceptance criteria, tasks, and testing notes after researcher validation | Codex |
| 2025-09-27 | 0.3     | Re-scoped shared client to remain on Chat Completions while retaining guardrails and retry guidance | Codex |

## Dev Agent Record
### Agent Model Used
- GPT-5 Codex (CLI)

### Debug Log References
- `pytest tests/unit/cli/test_openai_client.py tests/unit/cli/test_openai_probe.py tests/unit/config/test_openai_settings.py tests/integration/cli/test_openai_probe_cli.py`

### Completion Notes List
- Added `src/cli/openai_client.py` shared wrapper centralising chat and embedding helpers with telemetry, retries, redaction, and fallback awareness.
- Extended `config.settings.OpenAISettings` with retry/backoff/fallback configuration fields and new environment variable guardrails.
- Refactored `cli.diagnostics.run_openai_probe` to consume the shared client while preserving artefact/metrics schema and CLI surface.
- Documented new workflow knobs (`OPENAI_MAX_ATTEMPTS`, `OPENAI_BACKOFF_SECONDS`, `OPENAI_ENABLE_FALLBACK`) and shared-client responsibilities in architecture shards and `.env.example`.
- Added targeted unit/integration coverage for shared client retries, fallback flagging, embedding validation, and readiness-probe regression handling.

### File List
- .env.example
- docs/architecture/overview.md
- docs/architecture/coding-standards.md
- docs/stories/2.3.openai-shared-client.md
- src/cli/openai_client.py
- src/cli/diagnostics.py
- src/config/settings.py
- tests/unit/cli/test_openai_client.py
- tests/unit/cli/test_openai_probe.py
- tests/unit/config/test_openai_settings.py

## QA Results
### Risk Profile
- 2025-09-28: docs/qa/assessments/2.3-risk-20250928.md (risk_summary snippet available for gate)

### Test Design
- 2025-09-28: docs/qa/assessments/2.3-test-design-20250928.md (test_design snippet available for gate)

### Traceability
- 2025-09-28: docs/qa/assessments/2.3-trace-20250928.md (trace snippet available for gate)

### NFR Assessment
- 2025-09-28: docs/qa/assessments/2.3-nfr-20250928.md (nfr_validation snippet available for gate)

### PO Validation
- 2025-09-28: docs/qa/assessments/2.3-po-validation-20250928.md (Approved with follow-ups)

### QA Review
- 2025-09-28: docs/qa/assessments/2.3-review-20250928.md (Ready for Done)

## ðŸ”¬ Research & Validation Log (2025-09-26)

- **Researcher:** Dr. Evelyn Reed
- **Active Mode:** solo
- **Primary Artifact:** docs/stories/2.3.openai-shared-client.md
- **Summary:** Story scope and tasks align with Epic 2 guardrails, readiness probe reuse, and shared telemetry requirements. Minor follow-ups: convert the async support open question into a tracked task and schedule architecture-source-tree updates when the new client lands.

### Findings & Actions

| Priority | Area | Recommended Change | Owner / Reviewer | Confidence | Mode | Controls | Evidence Location | Sources |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Medium | Architecture docs | Create a follow-up action to update docs/architecture/source-tree.md once openai_client.py ships so structure guides stay current. | Scrum Master / Architect | Medium | solo | N/A | docs/stories/2.3.openai-shared-client.md#project-structure-notes | [docs/architecture/source-tree.md](docs/architecture/source-tree.md) |
| Medium | Async strategy | Capture a decision task answering the open question about async client usage to avoid rework later. | Product Owner / Dev Lead | Medium | solo | N/A | docs/stories/2.3.openai-shared-client.md#open-questions | [docs/architecture.md#openai-api](docs/architecture.md#openai-api) |

### Tooling Guidance

- **FOSS-first Recommendation:** Reuse existing `pytest` + `structlog` fixtures to exercise the shared client with faked OpenAI responses.
- **Paid Option (if required):** None â€” current stack remains open-source.
- **Automation / Scripts:** `PYTHONPATH=src pytest tests/unit/cli/test_openai_client.py tests/integration/cli/test_openai_probe_cli.py`

### Risk & Compliance Notes

- **Residual Risks:** Medium â€” async requirements unresolved could force refactor if future pipelines require concurrency.
- **Compliance / Control Mapping:** Internal observability standards that rely on Prometheus metrics continuity.
- **Monitoring / Observability:** Ensure shared client continues exporting metrics to `artifacts/openai/metrics.prom` for dashboards.
- **Rollback / Contingency:** Retain current probe implementation behind a feature flag until shared client adoption is verified.

### Follow-Up Tasks

- [ ] Log async client decision outcome and charter follow-up story if needed â€” Owner: Product Owner, Due: 2025-09-30
- [ ] Update architecture source tree shard after implementation merges â€” Owner: Dev Lead, Due: 2025-10-02

### Source Appendix

1. docs/prd.md â€” Epic 2 Models & Vectors requirements (Accessed 2025-09-26)
2. docs/architecture/overview.md â€” OpenAI readiness probe workflow (Accessed 2025-09-26)

## ðŸ”¬ Research & Validation Log (2025-09-28)

- **Researcher:** Dr. Evelyn Reed
- **Active Mode:** solo
- **Primary Artifact:** docs/qa/assessments/2.3-test-design-20250928.md
- **Summary:** Validated the updated test design against architecture guardrails, OpenAI rate-limit practices, Prometheus metric stability, and OWASP logging controls; captured follow-ups for metrics fixtures, chained retry scenarios, async strategy, secret masking, and allowlist audits. See docs/qa/assessments/2.3-researcher-validation-20250928.md.

### Findings & Actions

Refer to docs/qa/assessments/2.3-researcher-validation-20250928.md for the detailed findings table, tooling guidance, compliance mapping, and follow-up tasks registered on 2025-09-28.
