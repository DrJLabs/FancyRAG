# Story 1.3: Containerized Deployment

## Status
Ready for Review

## Story
**As a** deployment engineer,
**I want** the MCP server containerized and orchestrated with Neo4j,
**so that** the retrieval stack runs via Docker Compose across environments.

## Acceptance Criteria
1. Dockerfile builds Python image with FastMCP and project sources.
2. `docker-compose.yml` includes `mcp` service on `rag-net` with port 8080.
3. `docker compose up -d neo4j mcp` starts successfully and logs confirm server readiness.

## Tasks / Subtasks
- [x] Author Dockerfile for FastMCP service packaging application and dependencies (AC: 1) `[Ref: docs/epics/epic-1-hybrid-retrieval-foundation.md#primary-stories]`
  - [x] Base image on Python 3.12 slim variant and install Astral `uv` plus system deps required by `neo4j-graphrag` (AC: 1) `[Ref: docs/architecture/tech-stack.md#core-languages--runtime]`
  - [x] Copy `pyproject.toml`, `uv.lock`, and project sources into `/app`, perform a frozen install, and optimize layer caching (AC: 1) `[Ref: docs/architecture/source-tree.md]`
  - [x] Configure entrypoint to run `python servers/mcp_hybrid_google.py` with structured JSON logging to stdout (AC: 1) `[Ref: docs/architecture.md#operational-runbook-summary]`
  - [x] Document required environment variables and configure Compose `env_file` injection so secrets stay outside the image while `load_dotenv` gracefully handles missing files (AC: 1) `[Ref: docs/architecture.md#configuration--environment]`
- [x] Extend `docker-compose.yml` with `mcp` service built from the local image and bound to `rag-net` on port 8080 (AC: 2) `[Ref: docs/architecture.md#infrastructure-architecture]`
  - [x] Add `depends_on: neo4j`, share the `rag-net` bridge, and load `.env.local` via `env_file` for configuration alignment (AC: 2) `[Ref: docs/epics/epic-1-hybrid-retrieval-foundation.md#dependencies--inputs]`
  - [x] Implement Compose healthcheck polling `/mcp/health` (curl/wget) so readiness gates align with stack startup (AC: 2,3) `[Ref: docs/epics/epic-1-hybrid-retrieval-foundation.md#success-criteria]`
  - [x] Ensure container logs bubble up the `server.starting` message and FastMCP status for troubleshooting (AC: 3) `[Ref: docs/architecture.md#observability]`
- [x] Update operational tooling and docs to cover MCP container workflow (AC: 2,3) `[Ref: docs/architecture.md#deployment-pipeline]`
  - [x] Adjust Makefile targets (or add new ones) so `docker compose up -d neo4j mcp` is the default stack bring-up path (AC: 2) `[Ref: Makefile]`
  - [x] Document build/run instructions, healthcheck steps, and log verification in `README.md`, including canonical `docker build`/tag guidance to avoid image drift (AC: 3) `[Ref: docs/epics/epic-1-hybrid-retrieval-foundation.md#risks--mitigations]`
  - [x] Capture container runbook details (network, ports, env vars) in epic or architecture docs as needed (AC: 3) `[Ref: docs/architecture.md#operational-runbook-summary]`
- [x] Add automation to validate container startup and hybrid readiness (AC: 3) `[Ref: docs/architecture.md#testing-strategy]`
  - [x] Create integration smoke that builds the image, runs `docker compose up -d neo4j mcp`, seeds sample data (e.g., `make ingest` fixture), polls `/mcp/health`, and exercises `POST /mcp/search` with sample data (AC: 3) `[Ref: docs/epics/epic-1-hybrid-retrieval-foundation.md#success-criteria]`
  - [x] Integrate teardown/log capture to aid debugging and add guidance for CI adoption (AC: 3) `[Ref: docs/architecture.md#deployment-pipeline]`
  - [x] Record resulting coverage in QA risk/test design artifacts once implemented (AC: 3) `[Ref: docs/qa/assessments/1.2-risk-20251009.md#monitoring--follow-up]`

## Dev Notes

### Previous Story Insights
- Story 1.2 established `servers/mcp_hybrid_google.py` as the FastMCP entrypoint loading `.env.local` and constructing shared Neo4j state—reuse that module and keep runtime parity inside the container. `[Source: docs/stories/1.2.hybrid-mcp-server.md#dev-notes]`
- OAuth enforcement and latency guards already land in Story 1.2 tests; container startup must retain those defaults without bypassing authentication. `[Source: docs/qa/assessments/1.2-test-design-20251009.md#test-scenarios-by-acceptance-criteria]`

### Container Runtime Expectations
- Neo4j and MCP services share the `rag-net` bridge network so the server can reach `bolt://neo4j:7687`; Compose orchestration is the baseline runtime. `[Source: docs/architecture.md#infrastructure-architecture]`
- Local workflow requires `uv sync` followed by `docker compose up -d neo4j mcp`; container build must align with this sequence. `[Source: docs/architecture.md#deployment-pipeline]`
- Epic success criteria mandate running `docker compose up -d neo4j mcp` and passing `/mcp/health` plus sample search checks. `[Source: docs/epics/epic-1-hybrid-retrieval-foundation.md#success-criteria]`

### Configuration & Secrets
- Configuration loader demands `NEO4J_URI`, `INDEX_NAME`, embedding, OAuth, and MCP server variables; ensure they flow through `.env.local` (`env_file`) without baking secrets into the image. `[Source: src/fancryrag/config.py]`
- `.env.example` already enumerates container-ready defaults; mirror these when verifying Compose integration. `[Source: .env.example]`

### Build & Packaging Guidelines
- Python 3.12 with Astral `uv` is the approved runtime for packaging; align Dockerfile steps with that toolchain. `[Source: docs/architecture/tech-stack.md#core-languages--runtime]`
- Project layout expects application code under `src/`, entrypoints in `servers/`, and scripts/tests per source tree blueprint; copy only necessary artifacts into the image. `[Source: docs/architecture/source-tree.md]`

### Observability & Health
- MCP emits structured JSON logs via `fancryrag.logging_setup`; expose stdout/stderr so operators can monitor readiness and failures. `[Source: docs/architecture.md#observability]`
- Health validation should include calling `/mcp/health` and sample `/mcp/search` once data is ingested to confirm hybrid scoring semantics. `[Source: docs/epics/epic-1-hybrid-retrieval-foundation.md#validation--testing]`

### Testing
- Follow project testing strategy: unit tests remain under `tests/`, and add integration smoke that spins containers and exercises endpoints; incorporate into CI where feasible. `[Source: docs/architecture.md#testing-strategy]`
- Container smoke should reuse existing pytest/Testcontainers patterns or a dedicated script, asserting 1.5 s latency budget remains intact post-containerization. `[Source: docs/qa/assessments/1.2-risk-20251009.md#risk-based-testing-strategy]`

## Dev Agent Record

### Agent Model Used

- Codex GPT-5 (developer mode)

### Debug Log References

- `make smoke`
- `uv run pytest tests/test_compose_manifests.py`

### Completion Notes List

- Added a dedicated `docker-compose.smoke.yml` stack with an embeddings stub, wait script, and in-network pytest runner so smoke tests no longer bind host ports or reuse container names.
- Introduced `scripts/wait_for_bolt.py` and rewired the smoke pytest to seed Neo4j via the driver before exercising the MCP search tool.
- Updated the `Makefile` with deterministic `make smoke` and `smoke-logs` targets plus README guidance for the new workflow.
- Replaced the legacy container smoke test with `tests/smoke/test_stack.py` that uses the MCP client over the internal Compose network.
- Added static Compose validation tests plus `.dockerignore` assertions to prevent configuration drift and secret leakage.
- Created a container scan workflow and `make scan-image` target so image digest/secrets checks run automatically.
- Enforced the 1.5 s hybrid search latency budget inside the smoke test and parameterized the threshold for CI overrides.
- Recorded the PyYAML dev dependency required for Compose manifest parsing tests.

### File List

- Makefile
- README.md
- docker-compose.yml
- docker-compose.smoke.yml
- .github/workflows/container-scan.yml
- scripts/wait_for_bolt.py
- tests/test_compose_manifests.py
- tests/smoke/test_stack.py
- docs/stories/1.3.containerized-deployment.md
- pyproject.toml
- uv.lock

## Change Log
| Date       | Version | Description                     | Author |
|------------|---------|---------------------------------|--------|
| 2025-10-09 | 0.3     | Added compose schema tests, CI container scan workflow, and smoke latency enforcement | James |
| 2025-10-09 | 0.2     | Refined smoke automation with dedicated Compose stack and in-network pytest harness | James |
| 2025-10-09 | 0.1     | Initial draft of Story 1.3 (SM) | Bob |

## QA Results

### Review Date: 2025-10-09

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

Container orchestration is functional: Dockerfile builds cleanly, healthchecks and the smoke stack verify `/mcp/health` and `search`. However, automated safeguards are missing for two critical aspects—validation of the production `docker-compose.yml` manifest and secret hygiene for the built image—leaving regressions to be caught manually.

### Refactoring Performed

- None (review limited to analysis and QA documentation updates).

### Compliance Check

- Coding Standards: ✓ — Dockerfile/Compose definitions match project conventions.
- Project Structure: ✓ — New scripts/tests live under approved locations (`scripts/`, `tests/smoke/`).
- Testing Strategy: ✗ — No coverage protecting the production Compose manifest or enforcing secret scans/latency thresholds.
- All ACs Met: ✓ — Implementation satisfies AC1–AC3, but AC2 lacks regression tests.

### Improvements Checklist

- [ ] Add CI validation for `docker-compose.yml` (schema or unit test covering `services.mcp`).
- [ ] Build MCP image in CI, record digest, and run secret scanning (trivy/gitleaks).
- [ ] Measure `/mcp/search` latency within the smoke workflow and assert ≤1.5 s budget.
- [ ] Add drift detection or shared fragments for `docker-compose.yml` and `docker-compose.smoke.yml`.

### Security Review

Smoke Compose hardcodes a static token and there is no automated image secret scan; introduce CI scanning and clear guidance to prevent SEC-002 regressions.

### Performance Considerations

Smoke test exercises functionality but omits latency assertions; add timing to ensure containerized hybrid search stays within the documented SLA.

### Files Modified During Review

- docs/qa/assessments/1.3-trace-20251009.md
- docs/qa/assessments/1.3-nfr-20251009.md
- docs/qa/gates/1.3-containerized-deployment.yml

### Gate Status

Gate: CONCERNS → docs/qa/gates/1.3-containerized-deployment.yml
Risk profile: docs/qa/assessments/1.3-risk-20251009.md
NFR assessment: docs/qa/assessments/1.3-nfr-20251009.md

### Recommended Status

✗ Changes Required - See unchecked items above
