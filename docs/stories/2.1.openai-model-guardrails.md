# Story 2.1: OpenAI Model Configuration Guardrails

## Status
Ready for Dev Handoff

## Story
**As a** GraphRAG platform integrator,
**I want** model configuration and validation guardrails for OpenAI chat and embedding calls,
**so that** generation and vector workflows stay reliable before we scale ingest and retrieval pipelines.

## Acceptance Criteria
1. Centralize OpenAI chat configuration in `src/config/settings.py` (or equivalent) so the default model is `gpt-4.1-mini`, fallback guidance for `gpt-4o-mini` is documented for throughput emergencies, and unsupported values raise actionable errors while logging the actor and supplied value. [Source: docs/prd.md#epic-2---models--vectors] [Source: docs/architecture/overview.md#environment-configuration] [Source: docs/architecture.md#openai-api]
2. Provide an embeddings helper that enforces `text-embedding-3-small` with 1536-dimension vectors, failing fast when the returned length differs, emitting remediation guidance without exposing secrets, and allowing intentional overrides when alternate embedding dimensions are explicitly configured. [Source: docs/prd.md#epic-2---models--vectors] [Source: docs/prd/requirements.md#functional-requirements] [Source: docs/architecture.md#vectorrecord]
3. Instrument CLI calls to capture latency (ms) and token usage for both chat and embedding requests, export Prometheus/Grafana-compatible metrics with alert thresholds, and document expected cost/latency envelopes for the `gpt-4.1-mini` baseline versus the `gpt-4o-mini` fallback so operators understand trade-offs. [Source: docs/prd.md#epic-2---models--vectors] [Source: docs/architecture/coding-standards.md#logging-guidance] [Source: docs/architecture.md#openai-api]
4. Automated tests cover default/override paths (chat + embeddings), invalid model rejection, embedding dimension enforcement including custom and rejected overrides, and metrics logging behaviour using pytest doubles (no live API calls). [Source: docs/architecture/coding-standards.md#testing-expectations]

## Tasks / Subtasks
- [x] Introduce `OpenAISettings` loader under `src/config/settings.py` that reads `.env` values, enforces default `gpt-4.1-mini`, documents fallback behaviour to `gpt-4o-mini`, and blocks other inputs with structured error messaging. (AC: 1) [Source: docs/architecture/source-tree.md#source-tree-blueprint] [Source: docs/architecture/overview.md#environment-configuration]
  - [x] Emit structured logs (operation, model, override flag) via `structlog` when overrides are used so telemetry captures actor decisions. (AC: 1,3) [Source: docs/architecture/coding-standards.md#logging-guidance]
- [x] Add `ensure_embedding_dimensions()` utility (e.g., `src/cli/utils.py`) that asserts embeddings return length 1536, supports intentional overrides when a different dimension is configured, and raises `ValueError` with remediation tips when mismatched. (AC: 2) [Source: docs/architecture.md#vectorrecord]
  - [x] Wire the helper into existing diagnostics or forthcoming vector CLI entrypoint to surface failures with actionable guidance. (AC: 2) [Source: docs/architecture.md#cli-orchestrator]
- [x] Extend CLI instrumentation (diagnostics placeholder or new telemetry module) to track latency + token metrics for chat and embedding calls, persisting granular fields in structured logs, and emit Prometheus-exportable metrics with Grafana alert thresholds. (AC: 3) [Source: docs/architecture/coding-standards.md#logging-guidance]
  - [x] Update documentation (`docs/architecture/overview.md` or dedicated shard) summarizing expected cost/latency for `gpt-4.1-mini` baseline and `gpt-4o-mini` fallback, including retry/backoff guardrails and alert expectations. (AC: 3) [Source: docs/prd.md#epic-2---models--vectors]
- [x] Create pytest coverage: unit tests for `OpenAISettings`, unit tests for `ensure_embedding_dimensions()` (happy path + failure + override dimension + override misuse rejection), and integration-style tests stubbing OpenAI responses to confirm logging/metrics. (AC: 4) [Source: docs/architecture/coding-standards.md#testing-expectations]
  - [x] Provide fixtures that fake embeddings payloads without hitting external APIs, aligning with testing guidance. (AC: 4) [Source: docs/architecture/coding-standards.md#testing-expectations]

## Dev Notes

<!-- override-note -->
Warning: override of incomplete story status executed.
- Timestamp (UTC): 2025-09-25T19:01:32.663606+00:00
- Actor: ScrumMaster
- Prior Story: 1.4
- Prior Status: Ready for Review
- Reason: Proceeding to Epic 2 while Story 1.4 awaits review

### Previous Story Insights
- Story 1.2 establishes `.env` defaults for `OPENAI_MODEL` and mentions optional `gpt-4.1-mini`; configuration validation must remain consistent with that template. [Source: docs/stories/1.2.environment-configuration-template.md#dev-notes]
- Story 1.3's diagnostics workflow already masks secrets and writes audit artifacts; reuse its structured logging approach when adding model telemetry. [Source: docs/stories/1.3.environment-validation-command.md#dev-notes]

### Data Models
- `VectorRecord` payloads require `vector: float[1536]` plus join metadata; embedding guardrails should assert this dimension to prevent ingestion drift. [Source: docs/architecture.md#vectorrecord]

### API Specifications
- OpenAI API base URL `https://api.openai.com/v1` with bearer token auth; chat completions use `POST /v1/chat/completions`, embeddings use `POST /v1/embeddings`. [Source: docs/architecture.md#openai-api]
- Respect retry limits (max 5) and log token usage per request to honour cost-control guidance. [Source: docs/architecture/coding-standards.md#critical-rules]

### Component Specifications
- CLI orchestrator coordinates ingest, upsert, and search commands; configuration helpers should integrate cleanly with upcoming vector pipeline commands. [Source: docs/architecture.md#cli-orchestrator]
- Diagnostics command can host interim checks before dedicated vector CLI arrives. [Source: docs/architecture/overview.md#workspace-verification]

### File Locations
- Configuration modules belong under `src/config/`; CLI helpers live under `src/cli/`. Tests mirror layout in `tests/unit/config/` and `tests/unit/cli/`. [Source: docs/architecture/source-tree.md#source-tree-blueprint]

### Testing Requirements
- Use `pytest` for unit and integration coverage, with fixtures that stub OpenAI responses instead of making remote calls. [Source: docs/architecture/coding-standards.md#testing-expectations]
- Ensure tests mask secrets and exercise failure paths for unsupported models and incorrect embedding lengths. [Source: docs/architecture/coding-standards.md#critical-rules]

### Technical Constraints
- Remain on Python 3.12 with `openai` SDK 1.x and `structlog` for logging; enforce retry/backoff caps (â‰¤5) when capturing latency metrics. [Source: docs/architecture/tech-stack.md#technology-stack] [Source: docs/architecture/coding-standards.md#critical-rules]
- Avoid introducing new external CLIs; rely on existing Python dependencies noted in the tech stack. [Source: docs/architecture/tech-stack.md#technology-stack]

### Project Structure Notes
- Architectural blueprint anticipates additional modules (`src/config/settings.py`, `src/cli/vectors.py`); align new files with documented tree and update blueprint if layout diverges. [Source: docs/architecture/source-tree.md#source-tree-blueprint]

### Documentation Alignment
- Epic 2 expects operators to understand cost/latency implications; provide updates in PRD shards or architecture docs to keep runbooks consistent. [Source: docs/prd.md#epic-2---models--vectors]
- Grafana alert playbook maintained at `docs/alerts/openai-telemetry.yml`; CI guard enforces freshness and thresholds. [Source: docs/qa/assessments/2.1-test-design-20250925.md]

### Secrets Handling
- Log structured metrics without exposing API keys or raw prompts; follow secret-masking standards already applied in diagnostics. [Source: docs/architecture/coding-standards.md#critical-rules]

### Data Pipelines
- Future vector upsert pipeline relies on accurate embeddings and join metadata; this story should set validation hooks before ingestion logic is added. [Source: docs/architecture.md#vector-upsert-service]

### Open Questions
- None identified; confirm with Product Owner if additional model tiers (e.g., `gpt-4.1`) need inclusion before implementation.

## Testing
- Unit test `OpenAISettings` defaulting to `gpt-4.1-mini`, documenting fallback to `gpt-4o-mini`, and rejecting unsupported inputs with explicit messages. (AC 1,4)
- Unit test `ensure_embedding_dimensions()` for 1536-length success, mismatch failure with remediation text, intentional override dimension flow, and override misuse rejection. (AC 2,4)
- Integration-style test simulating chat + embedding calls via stubs to verify latency/token metrics logged, p95 histogram buckets populated, Prometheus-exportable fields emitted, and no secrets exposed. (AC 3,4)
- Unit logging test to confirm telemetry formatter redacts `api_key`, `authorization`, and `bearer` tokens in structured metrics/logs. (AC 3,4)
- Unit test ensuring Grafana thresholds remain in sync (`tests/unit/alerts/test_openai_alert_thresholds.py`). (AC 3)
- Documentation lint/test to ensure cost/latency guidance references baseline vs fallback models alongside alert thresholds. (AC 3)

## Change Log
| Date       | Version | Description                                        | Author        |
|------------|---------|----------------------------------------------------|---------------|
| 2025-09-25 | 0.1     | Initial draft of Story 2.1 created                 | Bob           |
| 2025-09-25 | 0.2     | QA, researcher validation, and PO sign-off logged | Quinn & Sarah |
| 2025-09-25 | 0.3     | Implemented configuration/telemetry guardrails and tests | James (Dev)   |
| 2025-09-25 | 0.4     | Added Grafana alert playbook automation and updated QA gate | Quinn & James |

## Dev Agent Record
### Agent Model Used
GPT-5 Codex (CLI)

### Debug Log References
* Ran `pytest tests/unit/config/test_openai_settings.py tests/unit/cli/test_utils.py tests/integration/test_openai_telemetry.py tests/unit/config/test_env_template.py`

### Completion Notes List
* Implemented `OpenAISettings` loader with chat allowlist, embedding overrides, and structured logging.
* Added embedding validation helper and Prometheus-compatible telemetry exporters with secret redaction safeguards.
* Updated `.env` template and architecture/PRD docs to reflect GPT-4.1-mini baseline.
* Added pytest coverage for configuration, embeddings, telemetry metrics, env template defaults, and Grafana alert thresholds.
* Established Grafana alert playbook (`docs/alerts/openai-telemetry.yml`) with CI freshness check.

### File List
* .env.example
* docs/alerts/openai-telemetry.yml
* docs/architecture.md
* docs/architecture/overview.md
* docs/prd.md
* docs/prd/requirements.md
* docs/qa/assessments/2.1-po-validation-20250925.md
* docs/qa/assessments/2.1-researcher-validation-20250925.md
* docs/qa/assessments/2.1-risk-20250925.md
* docs/qa/assessments/2.1-test-design-20250925.md
* docs/stories/2.1.openai-model-guardrails.md
* requirements.lock
* src/cli/telemetry.py
* src/cli/utils.py
* src/config/__init__.py
* src/config/settings.py
* tests/integration/test_openai_telemetry.py
* tests/unit/cli/test_utils.py
* tests/unit/config/test_env_template.py
* tests/unit/config/test_openai_settings.py

## QA Results
### Risk Profile
- 2025-09-25: docs/qa/assessments/2.1-risk-20250925.md (risk_summary snippet available for gate)

### Test Design
- 2025-09-25: docs/qa/assessments/2.1-test-design-20250925.md (test_design snippet available for gate)

### Traceability
- 2025-09-25: docs/qa/assessments/2.1-trace-20250925.md (coverage summary + gaps)

### NFR Assessment
- 2025-09-25: docs/qa/assessments/2.1-nfr-20250925.md (security/performance/reliability/maintainability)

### PO Validation
- 2025-09-25: docs/qa/assessments/2.1-po-validation-20250925.md (Approved for development)

### Gate Status
- Gate: PASS â†’ docs/qa/gates/2.1-openai-model-configuration-guardrails.yml

## ðŸ”¬ Research & Validation Log
- 2025-09-25: Reviewed Epic 2 requirements and architecture OpenAI integration sections to scope configuration guardrails.
- 2025-09-25: Research validation confirmed test matrix coverage for GPT-4.1 baseline, embedding dimension overrides, Prometheus telemetry, and secret masking. See docs/qa/assessments/2.1-researcher-validation-20250925.md.
