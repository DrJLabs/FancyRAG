# Research Validation: Story 2.3 Test Design

Date: 2025-09-28
Researcher: Dr. Evelyn Reed (Research & Validation Specialist)
Mode: solo

## ðŸ”¬ Research & Validation Log (2025-09-28)

- **Researcher:** Dr. Evelyn Reed
- **Active Mode:** solo
- **Primary Artifact:** docs/qa/assessments/2.3-test-design-20250928.md
- **Summary:** Reviewed the Story 2.3 test matrix against architecture guardrails for the shared OpenAI client, OpenAI rate-limit best practices, Prometheus metric stability requirements, and OWASP logging standards. Overall coverage is strong, with emphasis on allowlist enforcement, embedding validation, retry/backoff tuning, and artifact parity. Additional safeguards are recommended to manage OpenAI SDK churn, async adoption, and telemetry fixture drift.

### Findings & Actions

| Priority | Area | Recommended Change | Owner / Reviewer | Confidence | Mode | Controls | Evidence Location | Sources |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| High | Readiness probe regression | Capture golden fixtures for metrics/artifacts before refactor and extend 2.3-INT-007 to diff against them automatically, guaranteeing dashboards remain stable. | Dev / QA | High | solo | SRE Golden Signals | docs/qa/assessments/2.3-test-design-20250928.mdÂ§AC2 | [docs/architecture/overview.md#openai-readiness-probe](docs/architecture/overview.md#openai-readiness-probe); [Grafana Labs (2023)](https://grafana.com/blog/2023/09/12/how-to-choose-prometheus-histogram-buckets/) |
| High | Rate-limit resilience | Augment 2.3-UNI-010 with scenarios covering chained `Retry-After` headers and max-attempt exhaustion to align with OpenAI guidance on 429 handling. Ensure telemetry logs retry outcomes. | Dev / QA | Medium | solo | CIS Controls 16.12 | docs/qa/assessments/2.3-test-design-20250928.mdÂ§AC3 | [OpenAI Rate Limits (2024)](https://platform.openai.com/docs/guides/rate-limits) |
| Medium | Async migration | Add exploratory test or TODO capturing decision once async client strategy is defined, preventing future refactors from bypassing wrapper guardrails. | Product Owner / Architect | Medium | solo | ISO 25010 Maintainability | docs/stories/2.3.openai-shared-client.md#open-questions | [docs/architecture.md#openai-api](docs/architecture.md#openai-api) |
| Medium | Secret masking coverage | Extend 2.3-UNI-014 to include new headers (`x-openai-client`, `authorization`, `bearer`) and prompts with JSON payloads to satisfy OWASP logging controls. | Dev / QA | Medium | solo | OWASP ASVS 8.1 | docs/qa/assessments/2.3-test-design-20250928.mdÂ§AC4 | [OWASP (2021)](https://owasp.org/www-project-proactive-controls/v3/en/c6-implement-logging-and-monitoring) |
| Medium | Allowlist audit automation | Schedule periodic script (weekly) verifying wrapper allowlist against OpenAI model catalog to avoid regressions when new GPT-4.1 variants release. | QA | Medium | solo | CIS Controls 8.2 | docs/qa/assessments/2.3-test-design-20250928.mdÂ§AC1 | [OpenAI Model Catalog (2025)](https://platform.openai.com/docs/models) |

### Tooling Guidance

- **FOSS-first Recommendation:** Use `pytest` with `prometheus_client` registry snapshots and `structlog-testing` helpers; consider `respx` or `httpx` mocks to simulate OpenAI HTTP responses without paid tooling.
- **Paid Option (if required):** Datadog synthetic monitors for OpenAI retries, only if Prometheus dashboards lag. Evaluate cost ceilings before adoption.
- **Automation / Scripts:** Add CI job `pytest -k "openai_client and readiness_probe" --snapshot-update` guarded by reviewers to keep fixtures accurate while preventing accidental drift.

### Risk & Compliance Notes

- **Residual Risks:** OpenAI SDK updates could introduce new response fields requiring additional masking; async adoption remains unresolved and may alter retry semantics. Monitor OpenAI release notes monthly.
- **Compliance / Control Mapping:** Aligns with CIS Controls 16 (Application Software Security) and OWASP ASVS logging controls. Ensures observability remains audit-ready for SOC2 traces.
- **Monitoring / Observability:** Maintain Prometheus dashboards with latency, retry, and fallback counters; add alert on repeated fallback usage to detect upstream outages.
- **Rollback / Contingency:** Retain current readiness probe implementation behind feature flag until shared client passes all regression tests; fallback to legacy client if telemetry diff guards fail.

### Follow-Up Tasks

- [x] Generate metrics/artifact golden fixtures pre-refactor and wire into 2.3-INT-007 â€” Owner: QA, Completed: 2025-09-29
- [x] Expand rate-limit test coverage for chained `Retry-After` scenarios â€” Owner: Dev, Completed: 2025-09-29
- [x] Decide async client strategy and document adoption plan â€” Owner: Product Owner, Completed: 2025-09-29
- [x] Extend secret masking test data set (headers + JSON prompts) â€” Owner: Dev, Completed: 2025-09-29
- [x] Schedule weekly allowlist audit script in CI â€” Owner: QA, Completed: 2025-09-29

### Source Appendix

1. Neo4j GraphRAG Architecture Overview â€” OpenAI Readiness Probe workflow. Accessed 2025-09-28.
2. Grafana Labs. "How to choose Prometheus histogram buckets." Accessed 2025-09-28.
3. OpenAI Platform. "Rate limits guidance." Accessed 2025-09-28.
4. OWASP Foundation. "C6: Implement Logging and Monitoring." Proactive Controls v3. Accessed 2025-09-28.
5. OpenAI Platform Documentation. "Models." Accessed 2025-09-28.
