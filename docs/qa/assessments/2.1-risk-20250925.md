# Risk Profile: Story 2.1

Date: 2025-09-25
Reviewer: Quinn (Test Architect)

## Executive Summary
- Total Risks Identified: 4
- Critical Risks: 0
- High Risks: 2
- Risk Score: 73/100

Story 2.1 introduces centralised OpenAI configuration, telemetry, and embedding validation. Primary risks involve enforcing the GPT-4.1 mini baseline with safe fallbacks, preventing dimension drift in embeddings, and ensuring metrics surface latency/cost anomalies without leaking sensitive payloads. Sources: docs/stories/2.1.openai-model-guardrails.md, docs/architecture/coding-standards.md, docs/architecture/overview.md, docs/prd.md#epic-2---models--vectors.

## Risk Distribution
### By Category
- Technical: 1 risk (High: 1)
- Data: 1 risk (High: 1)
- Operational: 1 risk (High: 0)
- Security: 1 risk (High: 0)

### By Component
- `src/config/settings.py`: 1 risk
- `src/cli/utils.py` / vector helpers: 1 risk
- Telemetry/metrics pipeline: 1 risk
- Logging/observability stack: 1 risk

## Risk Matrix

| Risk ID  | Description                                                                     | Probability | Impact | Score | Priority |
|----------|---------------------------------------------------------------------------------|-------------|--------|-------|----------|
| TECH-210 | Baseline model enforcement fails, allowing unsupported OpenAI models in prod    | Medium (2)  | High (3) | 6     | High |
| DATA-210 | Embedding helper misses dimension drift, corrupting vector store ingestion       | Medium (2)  | High (3) | 6     | High |
| OPS-210  | Telemetry alert thresholds drift without regular review, reducing visibility into latency/cost spikes | Low (1) | Medium (2) | 2 | Low |
| SEC-210  | Telemetry logs expose prompt fragments or secrets                               | Low (1)     | High (3) | 3     | Low |

## Detailed Risk Register

| Risk ID  | Category    | Description | Probability | Impact | Score | Mitigation | Residual Risk |
|----------|-------------|-------------|-------------|--------|-------|------------|----------------|
| TECH-210 | Technical   | Configuration loader permits arbitrary chat models, causing API errors, higher latency/cost, or unsupported behaviour. | Medium (2) | High (3) | 6 | Enforce allowlist (`gpt-4.1-mini` baseline, `gpt-4o-mini` fallback) with structured error messages, log actor overrides, add unit/contract tests verifying rejection paths. | Future OpenAI model rename requires list refresh. |
| DATA-210 | Data        | Embedding helper fails to detect non-1536 vectors when overrides are active, leading to ingestion failures or silent vector corruption. | Medium (2) | High (3) | 6 | Implement strict dimension checks with optional override configuration, add pytest coverage for mismatch + override flows, monitor vector upsert errors. | Custom clients might bypass helper when adding new pipelines. |
| OPS-210  | Operational | Grafana latency/token alerts could drift if playbook becomes stale. | Low (1) | Medium (2) | 2 | Playbook `docs/alerts/openai-telemetry.yml` enforced via CI freshness check (`tests/unit/alerts/test_openai_alert_thresholds.py`) keeps thresholds aligned; update when model performance shifts. | Alert fatigue if thresholds tuned poorly or review cadence ignored. |
| SEC-210  | Security    | Telemetry/log payloads accidentally include prompt or secret values. | Low (1) | High (3) | 3 | Reuse diagnostics masking utilities, enforce structured logging that redacts payloads, add unit tests ensuring sensitive fields are filtered. | Unknown fields in SDK responses may appear later. |

## Risk-Based Testing Strategy
- **Priority 1:** Unit tests for `OpenAISettings` allowlist enforcement and embedding helper mismatch detection (TECH-210, DATA-210).
- **Priority 2:** Integration test stubbing OpenAI responses to verify Prometheus metrics and alert thresholds (OPS-210).
- **Priority 3:** Logging tests ensuring masking of secrets within telemetry payloads (SEC-210).

## Risk Acceptance Criteria
- Must fix before release: TECH-210, DATA-210 (both high risk).
- Deploy with mitigation: None; monitor OPS-210 via scheduled playbook reviews.
- Accepted risks: None at this stage.

## Monitoring Requirements
- Prometheus exporters for latency/token/cost metrics with Grafana dashboard and alerts tuned to PRD cost envelopes; CI ensures thresholds stay current.
- Alert on embedding dimension mismatch occurrences and emit structured error events.
- Secret-scanning on telemetry/log outputs to ensure masking continues working as SDK evolves.

## Gate Snippet
```yaml
risk_summary:
  totals:
    critical: 0
    high: 2
    medium: 0
    low: 2
  highest: DATA-210
  recommendations:
    must_fix:
      - Land unit tests enforcing GPT-4.1-mini baseline and embedding dimension checks before merge.
      - Implement structured error messaging for unsupported models with actor logging.
    monitor:
      - Review Grafana alert playbook freshness every 6 months (enforced via CI check).
```

## Hook Line
Risk profile: docs/qa/assessments/2.1-risk-20250925.md
