# Requirements Traceability Matrix

## Story: 2.2 – OpenAI Readiness Probe CLI

### Coverage Summary
- Total Requirements: 4
- Fully Covered: 4 (100%)
- Partially Covered: 0 (0%)
- Not Covered: 0 (0%)

### Requirement Mappings

#### AC1: Provide a CLI probe that issues lightweight chat + embedding requests, writes sanitized artifacts, and confirms guardrails before exiting 0 on success.
- Coverage: FULL
- Test Mappings:
  - test_file: tests/integration/cli/test_openai_probe_cli.py::test_openai_probe_cli_generates_artifacts
    given: Probe executed via `python -m cli.diagnostics openai-probe`
    when: CLI runs with fallback model override
    then: Generates sanitized `probe.json`/`metrics.prom`, records fallback flag, and leaves repo clean
    coverage: integration
  - test_file: tests/unit/cli/test_openai_probe.py::test_openai_probe_success
    given: Stubbed OpenAI client with deterministic responses
    when: Probe executes without overrides
    then: Report captures success status, sanitized chat metadata, and artifact paths
    coverage: unit
  - test_file: tests/unit/cli/test_openai_probe.py::test_openai_probe_skip_live
    given: `--skip-live` flag provided
    when: Probe runs without external calls
    then: Emits placeholder artifacts documenting skip rationale
    coverage: unit
  - test_file: tests/unit/cli/test_sanitizer.py::test_scrub_object_scrubs_sensitive_keys
    given: Diagnostics sanitizer invoked on nested payloads
    when: Sensitive keys/values encountered
    then: Redacted output confirms artifact/log hygiene
    coverage: unit

#### AC2: Emit Prometheus-compatible metrics with default histogram buckets (100 ms–5 s), annotate fallback usage, and redact structured logs.
- Coverage: FULL
- Test Mappings:
  - test_file: tests/unit/cli/test_openai_probe.py::test_openai_probe_success
    given: Probe records chat/embedding telemetry
    when: Metrics exported for baseline model
    then: Prometheus text includes histogram buckets and token counters
    coverage: unit
  - test_file: tests/unit/cli/test_openai_probe.py::test_openai_probe_identifies_fallback
    given: Fallback chat model configured (`gpt-4o-mini`)
    when: Probe executes
    then: Report flags `fallback_used` and metrics/logs capture override context
    coverage: unit
  - test_file: tests/unit/cli/test_telemetry.py::test_latency_histogram_uses_expected_buckets
    given: Metrics registry initialised via factory
    when: Buckets enumerated
    then: Boundaries align with 100 ms–5 s SLO guidance
    coverage: unit
  - test_file: tests/unit/cli/test_sanitizer.py::test_scrub_object_handles_mixed_structures
    given: Complex payload with secrets and tuples
    when: Scrubber processes report data
    then: Output remains sanitized across nested types
    coverage: unit

#### AC3: Handle guardrail violations, API errors, and 429/`RateLimitError` responses with exponential backoff, remediation messaging, and non-zero exit codes.
- Coverage: FULL
- Test Mappings:
  - test_file: tests/unit/cli/test_openai_probe.py::test_openai_probe_rate_limit_retries
    given: Stubbed client raising rate limit twice
    when: Probe executes with max_attempts=3
    then: Exponential backoff applied (0.01→0.02) and success captured on third call
    coverage: unit
  - test_file: tests/unit/cli/test_openai_probe.py::test_openai_probe_rate_limit_failure
    given: Client always raises rate limit
    when: Probe exhausts retries
    then: Report status=failed with token-budget remediation guidance, exit code 1
    coverage: unit
  - test_file: tests/unit/cli/test_openai_probe.py::test_openai_probe_skip_live
    given: `--skip-live`
    when: Operator opts out of live calls
    then: Report records skip rationale for CI stability
    coverage: unit

#### AC4: Update documentation and add automated tests (unit + integration-style) covering success, fallback, failure, artifact/metric assertions, and redaction negative cases.
- Coverage: FULL
- Test Mappings:
  - test_file: tests/integration/cli/test_openai_probe_cli.py::test_openai_probe_cli_generates_artifacts
    given: CLI invoked end-to-end
    when: Artifacts produced
    then: Validates doc instructions (paths, fallback indicators, sanitized outputs)
    coverage: integration
  - test_file: tests/unit/cli/test_openai_probe.py (suite)
    given: Stubbed OpenAI client scenarios (success, fallback, rate-limit, skip)
    when: Probe logic exercised
    then: Assertions cover artifacts, metrics, remediation text, and fallback behaviour
    coverage: unit
  - test_file: tests/unit/cli/test_sanitizer.py (suite)
    given: Sanitizer handling nested structures and env-derived secrets
    when: Scrubbing report/log payloads
    then: Ensures redaction of API keys, bearer tokens, and prompt strings
    coverage: unit
  - test_file: tests/unit/cli/test_diagnostics.py::test_output_redacts_secret
    given: Shared sanitizer invoked from diagnostics printing
    when: Message contains `OPENAI_API_KEY`
    then: Output masks actual secret aligning with documentation guidance
    coverage: unit

### Critical Gaps
- None detected.

### Test Design Recommendations
1. Add future smoke test once vector upsert workflow consumes probe metrics to guarantee artifact schema compatibility.
2. Monitor histogram bucket alignment as operators adjust latency SLOs; expose bucket settings via configuration override if needed.

### Risk Assessment
- Probe exercises all acceptance scenarios under unit/integration coverage; residual risk is low provided CI continues to enforce rate-limit handling and sanitization tests.

---
Trace matrix: docs/qa/assessments/2.2-trace-20250925.md
