# Requirements Traceability Matrix

## Story: 2.1 â€“ OpenAI Model Configuration Guardrails

### Coverage Summary
- Total Requirements: 4
- Fully Covered: 4 (100%)
- Partially Covered: 0 (0%)
- Not Covered: 0 (0%)

### Requirement Mappings

#### AC1: Centralize OpenAI chat configuration so the default model is `gpt-5-mini`, fallback to `gpt-4o-mini` is documented, and unsupported values raise actionable errors while logging actor + supplied value.
- Coverage: FULL
- Test Mappings:
  - test_file: tests/unit/config/test_openai_settings.py::test_chat_model_allowlist
    given: `OPENAI_MODEL` environment variable is unset or set to fallback
    when: `OpenAISettings.load()` resolves chat model
    then: Baseline remains `gpt-5-mini`, override to `gpt-4o-mini` triggers structured override log
    coverage: unit
  - test_file: tests/unit/config/test_openai_settings.py::test_invalid_chat_model_logs_and_raises
    given: Unsupported model value is supplied
    when: `OpenAISettings.load()` executes
    then: Function logs `openai.chat.invalid_model` event and raises `ValueError`
    coverage: unit

#### AC2: Provide embeddings helper enforcing 1536-dimension vectors, failing fast on mismatch, guiding remediation, and honoring intentional overrides.
- Coverage: FULL
- Test Mappings:
  - test_file: tests/unit/cli/test_utils.py::test_embedding_happy_path
    given: Embedding vector of length 1536
    when: `ensure_embedding_dimensions()` validates vector
    then: Returns original vector without logging errors
    coverage: unit
  - test_file: tests/unit/cli/test_utils.py::test_embedding_override_accepts_custom_dimension
    given: Environment override sets dimensions to 3072
    when: Validation runs with override-present vector
    then: Emits `openai.embedding.override_applied` and accepts vector
    coverage: unit
  - test_file: tests/unit/cli/test_utils.py::test_embedding_override_mismatch_raises
    given: Override configured but vector length mismatches
    when: Helper validates vector
    then: Logs `openai.embedding.override_mismatch` and raises `ValueError`
    coverage: unit
  - test_file: tests/unit/cli/test_utils.py::test_explicit_override_argument_takes_precedence
    given: Caller supplies explicit override dimensions argument
    when: Helper validates vector
    then: Explicit override accepted while default path still guards mismatches
    coverage: unit

#### AC3: Instrument CLI calls to capture latency and token usage for chat and embedding requests, export Prometheus/Grafana-compatible metrics with alert thresholds, and document cost/latency envelopes for baseline vs fallback models.
- Coverage: FULL
- Test Mappings:
  - test_file: tests/integration/test_openai_telemetry.py::test_chat_metrics_record_and_export
    given: Chat telemetry observation via stubbed metrics registry
    when: Metrics are recorded for `gpt-5-mini`
    then: Histogram counters increment and structured log emits redacted payload
    coverage: integration
  - test_file: tests/integration/test_openai_telemetry.py::test_embedding_metrics_and_redaction
    given: Embedding telemetry observation via stubbed registry
    when: Metrics are recorded for `text-embedding-3-small`
    then: Histogram counts increment and log redacts sensitive keys (`api_key`, `authorization`, `bearer`)
    coverage: integration
  - test_file: tests/unit/alerts/test_openai_alert_thresholds.py::test_playbook_exists_and_has_expected_models
    given: Grafana playbook `docs/alerts/openai-telemetry.yml`
    when: Thresholds are parsed during CI
    then: Baseline/fallback latency and token budgets, review freshness, and panel UIDs are enforced
    coverage: unit

#### AC4: Automated tests cover default/override paths, invalid model rejection, embedding dimension enforcement (including override misuse), and metrics logging behaviour using pytest doubles.
- Coverage: FULL
- Test Mappings:
  - test_file: tests/unit/config/test_openai_settings.py::test_embedding_override_validation
    given: Various override values including invalid entries
    when: `OpenAISettings.load()` processes override
    then: Accepts positive values and rejects non-positive or non-integer overrides with structured logs
    coverage: unit
  - test_file: tests/unit/cli/test_utils.py (suite)
    given: Embedding validation helper under multiple scenarios
    when: Vectors validated with/without overrides
    then: Happy path passes; mismatches raise descriptive errors
    coverage: unit
  - test_file: tests/integration/test_openai_telemetry.py (suite)
    given: Telemetry metrics recorded for chat and embeddings
    when: Observations exported via Prometheus registry
    then: Metrics available for scraping and logs redact sensitive content
    coverage: integration

### Critical Gaps
- None detected.

### Test Design Recommendations
1. Maintain CI enforcement of playbook freshness (<6 months) to prevent stale thresholds.
2. Add future smoke test once vector CLI consumes telemetry module end-to-end.

### Risk Assessment
- All acceptance criteria now have deterministic coverage across unit and integration suites; residual risk is low provided the playbook check remains in CI.

---
Trace matrix: docs/qa/assessments/2.1-trace-20250925.md
