# Requirements Traceability Matrix

## Story: 2.3 â€“ OpenAI Shared Client Abstraction

### Coverage Summary
- Total Requirements: 4
- Fully Covered: 4 (100%)
- Partially Covered: 0 (0%)
- Not Covered: 0 (0%)

### Requirement Mappings

#### AC1: Provide a shared OpenAI client module centralising chat and embedding helpers with guardrails, telemetry, and GPT-4.1 allowlist/fallback enforcement.
- Coverage: FULL
- Test Mappings:
  - test_file: tests/unit/cli/test_openai_client.py::test_chat_completion_uses_default_model
    given: Shared client initialised with default settings
    when: `chat_completion()` executes against the stub client
    then: Baseline model `gpt-4.1-mini` is used, telemetry captured, and no fallback flagged
    coverage: unit
  - test_file: tests/unit/cli/test_openai_client.py::test_chat_completion_marks_fallback_when_override
    given: `OPENAI_MODEL` override to `gpt-4o-mini`
    when: Shared client executes chat completion
    then: Fallback path activates, telemetry logs override usage
    coverage: unit
  - test_file: tests/unit/cli/test_openai_client.py::test_embedding_dimension_mismatch_raises
    given: Embedding response with invalid dimensions
    when: Shared client validates embedding
    then: Dimension guardrail raises `OpenAIClientError` with remediation
    coverage: unit
  - test_file: tests/unit/cli/test_openai_client.py::test_retry_after_header_controls_backoff
    given: Rate limit responses containing `Retry-After`
    when: Shared client retries chat completion
    then: Backoff honours header value and doubles delay
    coverage: unit

#### AC2: Refactor the readiness probe to consume the shared client without changing CLI surface, artifacts, or metrics.
- Coverage: FULL
- Test Mappings:
  - test_file: tests/integration/cli/test_openai_probe_cli.py::test_openai_probe_cli_generates_artifacts
    given: CLI invocation of `python -m cli.diagnostics openai-probe`
    when: Probe runs using stubbed OpenAI package
    then: Report/metrics artefacts match previous schema and fallback flag recorded
    coverage: integration
  - test_file: tests/unit/cli/test_openai_probe.py::test_openai_probe_success
    given: Shared client stub returning successful responses
    when: `run_openai_probe()` executes
    then: Report marks success, metrics preserved, override flag false
    coverage: unit
  - test_file: tests/unit/cli/test_openai_probe.py::test_openai_probe_rate_limit_retries
    given: Shared client stub raising rate limits twice
    when: Probe runs with retry/backoff hooks
    then: Backoff durations recorded and probe succeeds
    coverage: unit

#### AC3: Surface configurable retries, backoff, fallback toggles, and document operator guidance for latency/cost expectations.
- Coverage: FULL
- Test Mappings:
  - test_file: tests/unit/config/test_openai_settings.py::test_max_attempts_override_logs_and_applies
    given: `OPENAI_MAX_ATTEMPTS` override set
    when: `OpenAISettings.load()` executes
    then: Override stored and structured log emitted
    coverage: unit
  - test_file: tests/unit/config/test_openai_settings.py::test_backoff_override_validation
    given: `OPENAI_BACKOFF_SECONDS` override
    when: Settings load
    then: Floating-point override accepted and logged
    coverage: unit
  - test_file: tests/unit/config/test_openai_settings.py::test_disable_fallback_keeps_default
    given: `OPENAI_ENABLE_FALLBACK=false`
    when: Settings load
    then: Fallback disabled while baseline enforced
    coverage: unit
  - test_file: tests/unit/cli/test_openai_client.py::test_retry_after_header_controls_backoff
    given: Retry override scenarios
    when: Shared client executes
    then: Backoff implementation leverages header and doubles delay
    coverage: unit

#### AC4: Add unit and integration tests covering fallback usage, embedding overrides, rate-limit handling, metrics emission, and log redaction from both helper and CLI entrypoints.
- Coverage: FULL
- Test Mappings:
  - test_file: tests/unit/cli/test_openai_client.py (suite)
    given: Shared client operations across chat/embedding pathways
    when: Tests simulate success, fallback, dimension mismatch, and rate-limit flows
    then: Telemetry, logging, and guardrails validated
    coverage: unit
  - test_file: tests/unit/cli/test_openai_probe.py (suite)
    given: `run_openai_probe()` invoked with flaky/always-rate-limited clients
    when: Probe handles success, retries, and failure
    then: Reports/metrics align with expectations and remediation guidance captured
    coverage: unit
  - test_file: tests/integration/cli/test_openai_probe_cli.py::test_openai_probe_cli_generates_artifacts
    given: CLI invocation using stubbed OpenAI package
    when: Probe runs end-to-end
    then: Metrics/artefact redaction verified, probe remains compatible with existing tooling
    coverage: integration

### Critical Gaps
- None detected; all acceptance criteria map to automated coverage.

### Test Design Recommendations
1. When ingest/export scripts adopt the shared client, extend integration smoke tests to assert identical metrics payloads.
2. Add golden fixture diff checks (per researcher recommendation) once metrics artefacts stabilise to detect schema drift automatically.

### Risk Assessment
- Implementation and tests mitigate previously identified high risks (TECH-230, DATA-230). Residual risk remains tied to future async strategy; track via follow-up task.

---
Trace matrix: docs/qa/assessments/2.3-trace-20250928.md
